---
title: <Day019> Naver Boostcamp AI Tech
date: 2024-08-30
layout: post
tags: [Naver Boostcamp, daily report]
---
### 회고
내가 가장 어려워 하는 부분은 모델의 backpropagation부분인 것 같다. 
### 새롭게 배운 내용

t=1에 입력된 x_t의 '철수'라는 데이터가 가중치와 tanh가씌워져서 h1으로 저장될 것이다.
-> h2 -> h3 -> ... -> ht에 까지 전달될 것이다.

- 문제점: RNN이 재귀적인 함수형태 즉 RNN 모델의 output벡터인 hidden state 벡터가 다음 타임스탭에서 동일 함수의 입력으로 들어간다는 특성으로 인해서 h1, h2와 같이 모든 타임스탭의 hidden state 벡터들은 어떤 공통된 demension 수로 이루어진 벡터일 것이다.
	- 이는 정보를 저장할 수 있는 공간의 크기로 생각해보면 입력 시퀀스가 쭉 길어지면서 점점더 많은 양의 정보를 인코딩하고 이를 ht라는 벡터에 저장해 놓아야 한다. 저장 공간의 개수는 항상 일정하게 정해져있다.
	- 어느 시점부터는 계속 증가하는 정보를 축적하여 담기에는 저장공간이 부족해질 수 있다.
- 특정 타임스탭에서 나타난 그 정보는 수많은 타임스탭을 거쳐서 forward propagation 되는 과정 중에 W_hh라는 선형변환을 타임스탭의 개수만큼 수행하므로서 해당 정보가 어떤 형태로든 변질될 수 있는 가능성이 있다.
- 


### 심화과제
- 오류 발생
```python
---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

[<ipython-input-24-ecf2749a78dd>](https://localhost:8080/#) in <cell line: 1>()
----> 1 cbow_set = CBOWDataset(train_tokenized)
      2 skipgram_set = SkipGramDataset(train_tokenized)
      3 print(list(cbow_set))
      4 print(list(skipgram_set))

[<ipython-input-20-ffb3b7568c78>](https://localhost:8080/#) in __init__(self, train_tokenized, window_size)
     20      21     #리스트를 텐서로 변환
---> 22     self.x = torch.LongTensor(self.x)  # (전체 데이터 개수, 2 * window_size) 전체데이터개수만큼 있고 각각 2*window_size만큼 토큰들을 갖고 있음
     23     self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)
     24 

ValueError: too many dimensions 'str'
```
- 텐서로 변환할 때 리스트 안에 문자열과 같은 데이터 타입이 포함되어 있어서 발생한다. PyTorch 텐서는 동일한 데이터 타입의 값들만 포함되어있어야한다.
- 문제에서 torch.LongTensor라고 명시되어 있으므로 이는 정수형을 전달해줘야하고, 나는 문자열로 잘못 전달했다.
- self.x와 self.y를 단어로 구성되게 만들었는데 단어의 인덱스 즉, 위에서 w2i를 이용해서 정수로 전달해줘야 한다.
#### 그러나.. 또 문제 발생
```python
---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

[<ipython-input-32-ecf2749a78dd>](https://localhost:8080/#) in <cell line: 1>()
----> 1 cbow_set = CBOWDataset(train_tokenized)
      2 skipgram_set = SkipGramDataset(train_tokenized)
      3 print(list(cbow_set))
      4 print(list(skipgram_set))

[<ipython-input-30-001534c4ba4e>](https://localhost:8080/#) in __init__(self, train_tokenized, window_size)
     23      24     #리스트를 텐서로 변환
---> 25     self.x = torch.LongTensor(self.x)  # (전체 데이터 개수, 2 * window_size) 전체데이터개수만큼 있고 각각 2*window_size만큼 토큰들을 갖고 있음
     26     self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)
     27 

ValueError: expected sequence of length 2 at dim 1 (got 3)
```
- torch.LongTensor를 생성할 때 입력된 리스트의 길이가 일정하지 않아서 발생..
- 고정된 길이의 벡터 2 * window_size를 가져야한다.
- 문장의 처음이나 끝에서 주변단어의 수가 부족할 때 context의 길이가 2 * window_size보다 작을 수 있으므로 0으로 패딩을 추가해줘야한다.

### 피어세션
- RNN에서 역전파 어떻게 되는건지
	- SOS~EOS한번이렇게 끝나는건데 한번한다고 모델이 완성되는ㄱ너 아님
- 합산 LOSS -> 평가에만 쓰이나??
- 평가하고 그래프에 표시용도로 하는건가 싶었음.

### 오피스아워_이도현 조교님
- 심화과제 리뷰
- 긍부정 vocab을 만드는 곳에 w2v을 쓸 수 있을까?
	- 행복을 주변단어로 학습시키면 불행, 좌절 이런 여러 감정이 들어갈 수도 있기 때문에 긍정 부정 단어집합을 나누기는 힘들지만 감정 전체적인 단어집합을 구하는 것은 가능할 것 같다.
- w2v에서 활성함수를 사용하지 않는이유?
	- 특별한 이유는 없다. 
- 활성함수를 사용하는이유?
	- 활성함수를 사용하지 않고 레이어를 깊게 쌓으면 그거는 소수의 linearlayer와 똑같은 역할을 한다. w2v이 simple한 아키텍처라고 봐도 되기 때문에 이런것을 사용하지 않고 있다.
- 가중치를 곱하면 무조건 relu, 활성화함수 거쳐야한다? 
	- 실제로는 그렇지 않다. transformer에서도 활성함수를 사용하지 않는 부분이 들어가기도 한다. 
- 결론: 활성함수 넣어줄 수 있다.
Q. 심화과제 SkipGramDataset과 관련한 질문이 있습니다.
torch.LongTensor로 변환하는 과정에서, 주석 부분에 '전체 데이터 개수'라고 되어있는데요.
전체 데이터 개수 * 2 * window_size라고 하는 게 더 맞지 않나 하는 생각이 들었습니다.

A. 전체데이터 개수라는 거 자체가 용어의 혼란이 있었던 것 같다. 

모델 사용하다 보면 size가 굉장히 중요하다. 새로운 모델을 사용할 때 데이터를 먼저 넣어보고 중간 layer에 모든 size의 데이터를 다 뽑아본다. 실습 코드 볼 때 사이즈 뽑아보는거 권장


Q. 그리고, CBOWDataset 클래스를 구현하는 과정에서 온전히 window_size * 2의 크기를 만족하는 경우에만 x, y에 추가를 해주셨는데, 대신에 부족한 부분을 -1과 같은 '패딩 토큰'으로 채우는 방식은 어떤 지 궁금합니다!
A. 결론으로는 당연히 사용할 수 있다. pad token이라는 것은 torch.sum이라는 부분에서 pad토큰에 대한 embedding vector만 sum에서 합쳐지지 않도록 구현하면 가능하다.
그러나 굳이 이렇게 하지않는이유는 이렇게만 해도 성능이 잘 나왔다. 

Q. 패딩을 안하고 cbow를 학습하면, 문장 초반에만 나오는 단어들(나는, 우리는, ...)은 학습되기 어려워 질 수도 있지 않나요?
A. 정말 맛있다. 이거면 정말이라는 단어를 왼쪽에 pad token이 있다면 학습되기 어려워질 수도 있다. 그런 이유때문에도 pad-token을 굳이 사용하지 않는다는 것 같다.


Q. hidden layer의 차원 수가 커지거나 작아지는 게 학습 성능과 관계가 있나요?
A. vocab size가 32000이면 hidden layer를 512로 가정하면 압축하고 다시 decode하는 구조라고 생각할 수 있다. hidden layer의 차원을 2라고 설정하면 32000를 2개로 압축하고 다시 펼치려고 하면 엄청나게 그런 큰 표현력을 가지기 힘들 것이다.

만약hiddenlayer가 엄청 크면 과적합 문제가 발생할 수도 있겠지.
hiddenlyaer의 크기가 학습성능과 관계가 있다.

RNN, seq2seq는 그 시대에는 hiddenlayer의 차원을 조절하는 것에 따라서 성능차이가 꽴낳이 났다.
하이퍼파라미터 튜닝과정이 ㅁ낳이 그래서 필요했었다.

챗지피티 현대 LLM시대에 와서는 hiddenlayer의 차원이 커지냐안커지느냐에 따라 성능 차이가 크게 없다. 현대 LLM 사이즈가 큰 모델이 요즘 사람들의 관심사. 이런 LLM에서 학습성능에 가장 많이 영향미친는 것은 데이터의 양, 품질, 학습양.

하이퍼파라미터 서치는 프로젝트할 때 굉장히 중요할 것 같다.

Q. 조교님께서는 chatGPT를 어떻게 활용하고 계시는 지 궁금합니다!
A. 챗지피티가 없으면 안되능 삶.
논문 번역할 때 많이 사용한다. 
나중에 취업하거나 대학원갈때 번역을 하면 챗지피티 특유의 번역투가 있다. 초안을 위해서 번역을 하는 편이고
그래서 chain.from_iterable()이런 함수들 . pytorch, transformer 이런 도큐먼트를 보기에는 시간이 없거든요. 이런 상황인데 어떤 사용할만한 함수가 없을까? 물어보면 생각보다 추천을 잘해준다. 
코드짤때도 너무 귗낳은 코들르 짤때는 짜달라고 하기도 ㅎ나다.
json 파일을 로드하거나 save하거나 이럴 때 
코드를 짜거나 짜달라고 하거나 딥러닝 관련 질문을 물어보거나 할 때는 이중체크는 당연히 해야한다.

문장을 예쁘게 다듬을 떄 많이 사용한다. 슬랙에 출석 요정. 
그것도 대충 내용만 적고 챗지피티한테 이쁘게 다듬어달라고 했었다. 프로제그 보고서 쓸 때 많이 활용하면 좋지 않을 까

Q. 멀티 모달을 공부하고 싶은데 학습을 하기 위한 접근성이 너무 어려운것 같은데 사전에 공부해야 할것, 참고해야 할 논문등 멀티모달을 다루기 위한 접근 방법을 알려주실 수 있으신가요?
A. 석현 -> 멀티모달을 공부하고 싶은데 멀티모달을 어떻게 공부할까
1. GPT LLM을 뒤에서 배울 텐데 그걸 배우는게 첫번째
2. 지금은 NLP에 집중하기
3. LLM을 배워야하고 공부하고 싶은 사람들을 위해서 Introduction to vision language model 이런 논문이 있다 
https://ai.meta.com/research/publications/an-introduction-to-vision-language-modeling/
위의 비전 트랜스포머 이런거에 익숙해지면 위의 자료를 보는 것 추천
