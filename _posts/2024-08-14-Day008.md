---
title: <Day008> Naver Boostcamp AI Tech
date: 2024-08-14
layout: post
tags: [Naver Boostcamp, daily report]
---
### RNN (단점을 자세히 보고 Transformer가 왜 나왔나 살펴보기)

$$
h_t = f_w(h_(t-1), x_t)
$$

- Recurrent Neural Network : Recurrent하다?
    
    → 자신의 output이 자신의 input으로 들어간다는 것이다.
    
- 중요한 특징: RNN은 시퀀스 안의 각각 단어들을 같은 weight 파라미터로 처리한다.

장점:

- 다양한 길이의 input(가변적 길이)이 들어와도 x_t로 쪼개서 처리한다.

단점: 

- 병렬화(x_1, x_2, x_3 한번에 넣을 수 없)가 힘들다.(↔ sequential : 하나하나 들어가서 차례대로 처리)
- 1) vanishing gradient 문제 (기울기 소실)
- 2) long-range dependece (장거리 의존성) 모델링의 실패문제
    
    (→ 1)과 2)를 보완한 LSTM과 GRU → 긴 시퀀스 처리에 문제 → Attention model 생겨남)
    
- Many-to-many(input과 output크기가 같은) RNN은 입/출력 길이가 다르면 문제가 생긴다.(→ Seq2eq model이 생겨남)

---

### 1) Exploding/Vanishing Gradient 문제

RNN의 x_t에서 output(y_t hat)에서 loss를 계산하고 backpropagation을 수행한다.

- h_t에서 h_t-1로 backprop시 발생하는 W_hh는 t번 곱해진다 → Exploding문제와 Vanishing문제
- activation함수인 tanh가 미분되면 엄청 크거나 작은 값에서 기울기가 0이되어  vanishing gradients 문제 발생

→ 제대로 학습될 수 없다.

<aside>
💡 x가 정확히 1이 아닌 한(weight가 1인 아닌 한) x^k는 발산하거나 0으로 수렴한다.
k는? neural network의 depth이다. 
RNN에서의 k는 input sequence이다!(weight가 다 같기 때문)

</aside>

- fully connected 과정(W_xh를 x_t와 곱하고, W_hh를 h_t-1과 곱하여 더하는 과정)에서 backprop할때 W를 너무 많이 곱하게 되어 문제가 발생한다. → 이과정을 해결해야한다!

---

### LSTM

cell state(c_t)라는 고속도로를 뚫는다!

fully connected를 통과하지 않아도 전의 정보가 다음 정보로 흘러갈 수 있게 다.

- forget gate: 얼마나 잊어버릴 지를 컨트롤
- input gate: 얼마나 기억할 지를 컨트롤
- output gate: x_t, h_t, c_t를 combination해서 현재 상태를 얼마나 output 보낼건 지를 커트롤

⇒ RNN에서 long-term memory를 기억하지 못하는 단점을 보완했다. → 문장이 길 때 더 잘 저장 가능

- ex) forget gate = 1 :하나도 안 잊는다
    
    input gate = 0 : 모두 다 기억한다.
    
    RNN과 똑같이 작동하게 할 수 있다.
    
- 요즘 논문에서는 RNN, LSTM 워딩 혼용해서 쓴다. RNN방식 자체는 거의 사용 X

---

### GRU

LSTM에서 cell state와 hidden state도 있으니까 배울 것이 너무 많다!

→ h state를 두갈래로 나눠서 하나는 cell state처럼 쓰자. 

- LSTM과 거의 같은 방식으로 vanishing 문제 해결
- 적은 파라미터→ 더 빠른 계산과 LSTM보다 가볍게 시퀀스 모델링

## 피어세션

선형대수학
Least Square ~ 정규방정식 

## 마스터클래스_김수경 마스터
### QnA

Q. 인공지능은 어떻게 발전해갈 것이라고 생각하시나요? 수료생이 어떤 마음가짐과 자세로 이 기술을 활용하면 좋을지

A.  파이토치를 쓰는 인공지능 관련 직업은 많아질 것이다.

LLM이 가장 걱정이다. 코딩능력이 필요하지 않을 경향이 일어나고 있다.굉장히 빠르게 코딩능력이 필요없어지고 있다. 인공지능모델을 코딩하는 것을 배웠지만 직접 코딩하지 않고, “이런 구조를 가진 모델을 설계해라.” 라고 하면 만들어주는 모듈이 곧 만들어질 것이다.

5년 후에는 굉장히 다른 형태로 인공지능 연구나 산업이 발전하게 될 것 같다.

저절로 코딩 된 코드를 가지고 다른 분야에 적용하는 산업이 발전하게 될것같다. 로보틱스, 스마트시티, 재료, 기후연구.. 직접 모델을 짜지 않고, 특정 도메인의 지식이 있어야만 적용할 수 있는 많은 분야가 있을 것 같다.

Q. 전자공학, 재료, 자연과학…  많은 전공을 하신 이유 why?

A. 직업을 잘 찾고, 연봉이 높은 머신러닝으로 전향하게 되었다.

Q. 다른 전공이였는데 커리어를 어떻게 시작하셨는지, 도메인 지식을 머신러닝에 어떤방식으로 활용할까부터 시작하셨을 것 같은데 어땠는지?

A. 재료공학과 지식이 도움이 됐다. 이력에서 보았다시피 자연 과학을 연구하는 연구소였다. 자연과학의 배경도 있고, 머신러닝의 지식도 있으니까 나를 고용 했을 것이다. 

Q. 환경 기상 분야 GIS 이용한 것, 에너지 환경 쪽 취업 어떻게 할까
A. 제가 썼던 데이터는 굉장히 long-term climate future를 예측한 시공간적 데이터이다. 그런 연구는 GPU가 많이 필요하다. NVDIA나 google 지구온난화가 이루어지는 것은 기정사실이기 때문에 관련 앱을 만들거나 하는 것은 전망이 좋다고 생각한다.

Q. 미국대학원 유학, 어떤것을 준비하면 좋을까, 어떤 어려움이 있을지, 스토리

유학가고싶은건 고딩때부터 그랬다. 한국이 기회가 많이 제한되어있다고 생각한나. 

tech use, 새로운 기술 → 알다시피 선두주자는 미국에서 많이 이루어지고 있다. 대학교 때부터 유학준비를 했고, 머신러닝, AI 쪽으로 유학가려고 하면 요즘은 경쟁률이 굉장히 심할 것이다.

추천하는 것은 대학원에 입학 하려면 적어도 머신러닝 분야에서 탑티어는 아니더라도 세컨티어까지 페이퍼가 한두개는 있어야한다. 페이퍼를 만들기위해 국내 연구소에서 일하면서 페이퍼를 만들어야한다.

외국생활? 어려움 딱히없다. 조지아택, 콜롬비아 → 이대보다 학점따기 쉬웠다. 영어가 괜찮다면 어려움은 없을 것이다.

Q. 주위에서 본 AI resesracher, engineer중 똑똑하게 공부한사람, 진로 설정을 잘 한 사람?

A. 서울대에서 석박을 했는데 박사하면서 논문을 잘 써서 한명은 조지아텍으로 나가고, 스위스에 나간 사람이있다. 누구보다 publication이 많고, 노력, critical thinking 굉장히 훈련이 잘되어서 본받을 게 많은 사람들이다.

연구를 위해서 유학을 가는 것은 다시 한번 생각해봐라. 유학을 가다고해서 좋은 연구를 하는 건아니다. 굉장히 변수가 많다. 못된 교수, 좋은 사람, 신경안쓰는 사람 우리가 유학가기 전에는 이런 변수들을 알 수 없다. 학생들을 많이 자르기도 한다. 굉장히 변수가 많고, 위험한 선택이다. 국내 좋은 대학원에서 좋은 교수님 밑에서 안정적으로 박사를 하면 페이퍼도 많이 생기고, 페이퍼가 많으면 그 상태에서 영주권을 신청할 수도 있다. 박사영주권으로 취업한 사람도 있다. 훨씬 이방법이 더 안정적으로 성공할 수 있는 기회이다.

Q. XAI 분야 관련

A. 도메인마다 xai 평가방법이 다를 것 같다. 도메인따라 다르기보다 모델따라 다르다. convolution기반 모델을 썼다. transformer기반 모델이면 attention map을 그려서 input에 얼마나 기여를 했는지 볼 수 있다.


Q. 함수들의 수식을 보면서 어려움을 겪고 있다.

A. AI 엔지니어링, 프로그래밍 job은 수학을 그렇게 깊게 할 필요는 없다. 엔지니어링은 개발을 잘하면 된다.  

AI 엔지니어나 사이언티스트, 데이터 사이어스 직군 , 모델 만드는 직군 → 수학을 잘 이해해야한다. 기적인 수학을 이해해야한다.

Q. 대학원을 준비하는 대학생. 어떻게 논문 읽는 것을 어떻게 시작해야할까. 논문 추천.

A. CNN, RNN, Seq2Seq 

Attention is all you need

VIT → visual transformation

hugging face 사이트 ⇒ 블로그를 잘 보면 이해한 다음 최근에 나온 논문을 follow up 할 수 있다. 

부트캠프의 내용들을 다 소화해라 우선. 그리고 이해안되면 인터넷의 블로그를 읽어보고 논문을 읽어봐라. 

부트캠프 내용을 다 소화하자.!!!!

Q. 비디오 분야 어떻게 발전 ?

Visual Transfromer!!!!!!!!!!!!!!!를 요새 주로 쓰는 추세이다.

**캠퍼들에게 하고싶은 말**

부트캠프가 엄청 빡세다. 이걸 다 하면 내가 지금 가르치고 있는 인공지능학부애들보다 더 많이 배울 수 있을 것 같고, 산업에서 많은 걸 할 수 있을 것이다. 어떻게 보면 이 과정을 마치고 인공지능분야에서 이것저것 기회가 많을 것이다. 부트캠프 듣는 사람들 중에 큰일을 많이 하는 사람이 나왔으며 좋겠다.
