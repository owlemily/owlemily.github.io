---
title: <Day008> Naver Boostcamp AI Tech
date: 2024-08-14
layout: post
tags: [Naver Boostcamp, daily report]
---
### RNN (단점을 자세히 보고 Transformer가 왜 나왔나 살펴보기)

$$
h_t = f_w(h_(t-1), x_t)
$$

- Recurrent Neural Network : Recurrent하다?
    
    → 자신의 output이 자신의 input으로 들어간다는 것이다.
    
- 중요한 특징: RNN은 시퀀스 안의 각각 단어들을 같은 weight 파라미터로 처리한다.

장점:

- 다양한 길이의 input(가변적 길이)이 들어와도 x_t로 쪼개서 처리한다.

단점: 

- 병렬화(x_1, x_2, x_3 한번에 넣을 수 없)가 힘들다.(↔ sequential : 하나하나 들어가서 차례대로 처리)
- 1) vanishing gradient 문제 (기울기 소실)
- 2) long-range dependece (장거리 의존성) 모델링의 실패문제
    
    (→ 1)과 2)를 보완한 LSTM과 GRU → 긴 시퀀스 처리에 문제 → Attention model 생겨남)
    
- Many-to-many(input과 output크기가 같은) RNN은 입/출력 길이가 다르면 문제가 생긴다.(→ Seq2eq model이 생겨남)

---

### 1) Exploding/Vanishing Gradient 문제

RNN의 x_t에서 output(y_t hat)에서 loss를 계산하고 backpropagation을 수행한다.

- h_t에서 h_t-1로 backprop시 발생하는 W_hh는 t번 곱해진다 → Exploding문제와 Vanishing문제
- activation함수인 tanh가 미분되면 엄청 크거나 작은 값에서 기울기가 0이되어  vanishing gradients 문제 발생

→ 제대로 학습될 수 없다.

<aside>
💡 x가 정확히 1이 아닌 한(weight가 1인 아닌 한) x^k는 발산하거나 0으로 수렴한다.
k는? neural network의 depth이다. 
RNN에서의 k는 input sequence이다!(weight가 다 같기 때문)

</aside>

- fully connected 과정(W_xh를 x_t와 곱하고, W_hh를 h_t-1과 곱하여 더하는 과정)에서 backprop할때 W를 너무 많이 곱하게 되어 문제가 발생한다. → 이과정을 해결해야한다!

---

### LSTM

cell state(c_t)라는 고속도로를 뚫는다!

fully connected를 통과하지 않아도 전의 정보가 다음 정보로 흘러갈 수 있게 다.

- forget gate: 얼마나 잊어버릴 지를 컨트롤
- input gate: 얼마나 기억할 지를 컨트롤
- output gate: x_t, h_t, c_t를 combination해서 현재 상태를 얼마나 output 보낼건 지를 커트롤

⇒ RNN에서 long-term memory를 기억하지 못하는 단점을 보완했다. → 문장이 길 때 더 잘 저장 가능

- ex) forget gate = 1 :하나도 안 잊는다
    
    input gate = 0 : 모두 다 기억한다.
    
    RNN과 똑같이 작동하게 할 수 있다.
    
- 요즘 논문에서는 RNN, LSTM 워딩 혼용해서 쓴다. RNN방식 자체는 거의 사용 X

---

### GRU

LSTM에서 cell state와 hidden state도 있으니까 배울 것이 너무 많다!

→ h state를 두갈래로 나눠서 하나는 cell state처럼 쓰자. 

- LSTM과 거의 같은 방식으로 vanishing 문제 해결
- 적은 파라미터→ 더 빠른 계산과 LSTM보다 가볍게 시퀀스 모델링

## 피어세션

Least Square ~ 정규방정식
