---
title: <Day010> Naver Boostcamp AI Tech
date: 2024-08-19
layout: post
tags: [Naver Boostcamp, daily report]
---
## 학습내용

### Attention

**기존 seq2seq의 문제점**

seq2seq model에서 Encoder의 마지막 시퀀스에 입력의 모든 정보를 저장하게 된다. 만약 input이 책한권 이라면? single embedding으로 모든 정보를 저장하는 것이 불가능하여 앞의부분이 손실된다. → 하나의 벡터로 embedding하는 것의 한계.

**Attention의 mechnism**

- Encoder단계의 모든 hidden state를 Decoder의 input으로 고려하여 더 중요한 input에 집중한다.

$$
Attention(Q, K, V) = Attention값
$$

- Q(Query) : decoder의 time t에 대한 hidden state
    
    K(Keys): encoder의 hidden state(모든 t에 대한)
    
    V(Values): encoder의 hidden state(모든 t에 대한)
    
    - Q는 비교 기준/ K, V는 비교 대상이다.
    - seq2seq에서는 K와 V가 같은 의미이다.

<aside>
💡 1) Decoder의 한 단계의 hidden state(s_0)와 Encoder의 hidden state들 간의 내적을 한다.

→ 이것이 유사도이다.  input의 어떤 단어가 output의 hidden state와 가장 유사한지 내적을 통해 계산한다.

2) 내적값들을 softmax() 취하여 확률값으로 구한다.

→ 이를 통해 Attention coefficients(a_t)를 구한다.

3) 가장 유사도가 높은 weight을 decoding hidden state에 concatenation을 한다. 

4) fully connected를 통과하여 단어를 예측한다.

5) 문장 끝날때까지 반복한다.

</aside>

- 단어를 하나하나 뽑아서 end token이 나올 때까지 하기 때문에 many-to-many가 아니여도 된다.

---

### Transformers

- chatGPT의 근간이 되는 모델이다.
- 지금까지는 input하나에 어떤 weight를 곱해서 output인 y를 예측했다.
- self-attention
    - 사람: 각 element 사람 한명은 주위의 사회적 관계 정보를 통해 own representation을 개선할 수 있다.
    - 문장, 글: 각 element는 문장에 속한 단어이고, 각 단어들은 전체 문장의 context에 유기적으로 관련되어서 representation된다.

**Attention Mechanism과 같은 Transformer**

- Transformer가 attention과 다른점: Q, K, V를 만들 때 weight파라미터를 쓴다는 것이다.
- contextualize: 단어 x1을 그 문장 안 context를 이용해서 다시 representation learning을 한 것.

<aside>
💡 비디오에서 각 이미지들의 시퀀스인 x1, x2, x3, x4, x5를 생각해보자.
1) x1, x2, x3, x4, x5를 h1, h2, h3, h4, h5라는 hidden state로 embedding한다.
2) x1을 W_Q를 곱해서 Q1으로 보낸다.
3) 각각의 x에 W_K를 곱해서 K1, K2, K3, K4, K5를 구한다.
4) 각각의 x에 W_V를 곱해서 V1, V2, V3, V4, V5를 구한다.
5) Q1과 K1 dotproduct하여 유사도를 구한다.
Q1과 K2, Q1과 K3 … Q1과 K5 각각 모두 유사도를 구한다.
6) 각각의 유사도만큼 V를 집중하게 만들기 위해서(유사도가 큰것을 더 집중해서 보기 위해) 각각의 유사도와 V를 곱한다.
7) 나온 값들을 Weight average해서 W_o를 곱해서 z1을 예측한다.
⇒ x1, 첫번째 이미지에 대한 representation을 마쳤다. 이것을 모든 이미지 x2, x3.., x5에 다 한다.

</aside>

- 위의 과정에서 input은 각 time stamp의 이미지였지만 output z1부터 z5는 각각의 이미지 뿐 아니라 주변 이미지들과 어떻게 상호연관되어있는지를 반영한 어떤 contextualized된 representation이다.
- Transformer는 Attention모델의 좀 더 일반화된 모델이다.

---

### Transformer의 질문들

1. **비디오 분류, 문장분류, 회귀와 같은 작업은 Transformer를 어떻게 이용할까?**
- Transformer는 seq2seq 모델인데 어떻게 분류와 회귀를 진행하는가?
- Transformer의 모든 output z1~z5를 평균을 구한다음 다시 MLP만 얹어서 원하는 작업을 한다.
    - 평균을 구하는 것: 시퀀스가 짧고, 모든 단어가 비슷비슷하여 특정 단어가 유독 중요하지 않고, 모두 비슷한 중요도를 가질 경우에는 평균 구하는 것이 효과적.
    - → 그러나 대부분 이렇지 않다!
- output z1~z5의 또다른 aggregation방법
    - CLS(Classification token) 더미토큰을 입력시퀀스에 추가하여 embedding z0를 구한다.
    - z0를 가지고 classification이나 regression 작업에 이용
    - CLS토큰은 어떤 의미도 전달하지 않고, 모든 토큰depend된다.
1. **Transformer model을 어떻게 학습시키고, back prop을 어떻게 하는지?**
2. **RNN과 다르게 Transformer는 토큰의 순서가 무시된다. 문장은 순서가 중요해서 토크의 순서가 무시되면 안된다. 이를 해결하기 위해 어떻게 모델링 할 수 있을까?**

### BERT : Transformer의 발전된 형태

- Bidirectional Encoder Representations from Transformers
- Transformer의 Encoder만 가지고 와서 많은 데이터셋으로 pretraining 한 것이다.
- Self Supervised(label이 없어도 학습) 한다.
- BERT에서 training하는 방법이 여러가지 문장을 많이 넣는다. 여러 문장의 연속된 데이터를 input으로 넣는다.

**어떻게 Self Supervised 되는지 살펴보자.**

input의 token을 3가지를 이용해서 설계했다.

1. Token embedding→ 단어에 대한 embedding
    1. [CLS] : Classification token
    2. [SEP] : 문장의 끝을 표시하는 토큰
2. Segment embedding
    
    여러가지 문장을 input으로 넣기 때문에 각 토큰이 속한 문장이 어떤 문장인지 표시.
    
3. Position embedding
    
    Transformer의 구조에 속한 positional encoding(sinusoidal encoding)을 합쳐서 시퀀스의 순서 정보를 표시한 embedding
    

**BERT가 Self Supervised로 pretraining되는 작업에 2가지가 쓰임**

1. Masked Language Modeling(MLM) 작업 이용
    
    랜덤하게 어떤 단어를 마스킹한다. BERT 모델이 문제를 통해서 빈칸에 어떤 단어가 올 지를 맞추는 작업을 풀게 한다. → label이 필요없다. 문장이 주어지면 랜덤하게 어떤 단어를 마스킹하면 되기 때문
    
2. Next Sentence Prediction(NSP)
    
    BERT모델을 문장 여러개를 넣어서 embedding하는 작업을 풀게 했는데, 입력된 두개의 문장이 같은 문단에서 왔는지, 연속되지 않는 랜덤한 두 문장인지를 예측하게 했다.
    
    이 분류 작업을 BERT가 배우게 해서 BERT가 문장을 이해하는 이해도가 높아질 것이라고 가정했다.
    

### 컴퓨터 비전에서의 Transformer

- ViT: Vision Transformer(요즘 굉장히 많이 쓰인다 )
    
    CNN을 쓰지 않고 이미지를 통해 어떤 작업을 할때 Transformer를 쓴다.
    
    → **Transformer는 sequenced된 input을 처리하는 것인데 어떻게 이미지에 적용될 수 있지?**
    
    하나의 이미지를 패치로 여러조각으로 분할하여 나열한 뒤 단어 tokens와 같은 방식으로 처리한다.
    
- ViT는 극단적으로 큰 데이터셋에서 잘 작동한다는 논문이 있다.
    
    ViT는 CNN이 가지고 있는 inductive bias(공간적 근접성 및 위치 불변)를 가정하지 않는다. → ViT를 이용하면 이미지가 들어왔을 때 패치의 조각으로 자른다. 이미지를 자르면 이미지 내의 공간적 정보를 다 잊을 수 밖에 없다. 
    
    - 각 패치를 통해 학습하려면 굉장히 많은 데이터가 필요하다. → spatial locality를 ㅓㄴㅁ어서는 복잡한 사례를 모델링할 수 있어서 CNN보다 훨씬 뛰어난 성능을 발휘할 수 있다.
- 작은 데이터셋에서 ViT보다는 기존의 CNN이 더 잘 된다고 한다.
- ViT도 Position Embedding이 필요하다.
    - 이미지 내의 거리를 position embedding의 유사성을 통해 학습.
    - 패치가 어디에 위치했는지를 의도적으로 transformer에 알려줘야한다.
 

## 피어세션

선형대수학 스터디

Orthogonal Projection ~ 그람슈미트 직교화와 QR 분해해
