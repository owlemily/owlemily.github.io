---
title: <Day009> Naver Boostcamp AI Tech
date: 2024-08-16
layout: post
tags: [Naver Boostcamp, daily report]
---
## 학습내용

### Seqence-to-Sequence(seq2seq) Models

**기존 RNN(many-to-many) 문**

- 기존의 RNN은 many-to-many로 input과 output의 1:1관계를 가정한다.
- Machine translation의 경우: 번역하는 언어마다 단어 개수 길이가 다를 수 있고, 어순도 달라서 input과 output이 일대일로 매칭되지 않을 수 있다.
    
    → 즉, 입력에 기반해서 출력하는 것이 불가능하다고 볼 수도 있다. 
    

**seq2seq 모델의 단계:**

1) Encoder

2) Decoder

3) 1,2를 불러와서 seq2seq 모델로 만들기

**Encoder- Decoder Structure**

- Encoder 단계: input 문장의 전체적인 정보를 배운다.
    - Encoder의 마지막 hidden state에서는 전체 입력 시퀀스(전체문장→ 사람이 한 문장을 다 들은 거처럼) embedding을 갖고 있다.
    - 즉, Encoder는 문장을 읽어서 저장!
- Decoder 단계: 읽어들인 문장을 본격 번역한다.
    - Auto-Regressive: output으로 단어를 예측했을 때, 그 단어가 다음단계의 input으로 들어가는 과정. (생성형 AI가 Auto-Regressive한 방식으로 예측이 되고 있음)
    - Techer Forcing: 학습할 때 이용.
        
        초반 학습 단계에서는 학습이 안됐으니 첫 예측 단어 y_1 hat이 틀린 단어가 나올 수 있다. → 틀린 단어로 계속 하면 계속 다음 단어도 틀리게 나올 수 있다. 이를 방지하기 위해서 예측값 y_1 hat을 바로 다음 input에 넣지 않고 ground truth data인 y_t를 다음 input값으로 넣는다.
        
        모델이 쉽게 train할 수 있도록!
        
        - inference 단계(train이 끝나고 test할 때): 실제로 예측한 값을 넣어준다. 번역작업을 실제로 할 때는 ground truth값 정답값이 없기 때문이다.
- train 시킬 때 Encoder와 Decoder에서 나오 loss값을 이용하여 encoding과 decoding을 훈련시킨다. → Decoder에서 Encoder까지 전체 모델을 한꺼번에 학습한다.
- Encoder-Decoder Structure은 input길이, output길이 상관없이 잘 번역할 수 있는 특징이 있다.

**코드**

1) Encoder

```python
class EncoderLSTM(nn.Module):
	#def __init__ 생략
	
	def forward(self,x):
		#shape: [sequence length, batch size, embedding dims]
		embedding = self.dropout(self.embedding(x))
		#output shape: [sequence length, batch size, hidden_size]
		#hs, cs shape: [num_layerss, batch_size, hidden_size]
		outputs,(hidden_state, cell_state) = self.LSTM(embedding)
	
		return hidden_state, cell_state
```

2) Decoder

```python
class DecoderLSTM(nn.Module):
	#def __init__생략
	
	def forward(self, x, hidden_state, cell_state):
		x = x.unsqueeze(0) #shape of x: [1, batch_size]
		embedding = self.dropout(self.embedding(x)) #shape:[1, batch size, embedding dims]
		
		# outputs shape:[1, batch size, hidden_size]
		# hs, cs shape: [num_layers, batchsize, hidden_size] <- hs, cs from Encoder
		outputs, (hidden_state, cell_state) # shape: [1, batch_size, output_size]
		predictions = self.fc(outputs)#shape: [1, batch_size, output_size]
		predictions = predictions.queeze(0) # shape: [batch_size, output_size]
		
		return predictions, hidden_state, cell_state
```

→ dropout()함수: 과적합을 방지하기 위해 사용되는 정규화기법. Encoder와 Decoder 모두 dropout을 적용해서 임베딩 벡터에 노이즈를 추가하여 과적합을 방지하고, 모델이 더 일바화된 패턴을 학습하도록 한다. 

3) seq2seq

```python
class Seq2Seq(nn.Module):
	#def __init__생략
	
	def forward(self, source, target):
		batch_size = source.shape[1] #source shape:[input language seq len, num_sentences]
		target_len = target.shape[0] #target shape:[output language seq len, num_sentences]
		target_vocab_size = len(english.vocab)
		
		outputs = torch.zeros(target_len, batch_size, target_vocab_size)
		hs, cs = self.Encoder_LSTM(source)
		
		x = target[0] #Trigger token <SOS>; shape: [batch_size]
		
		for i in range(1, target_len):
			output, hs, c = self.Decoder_LSTM(x, hs, cs)
			output[i] = output
			x = output.argmax(1)
		
		return outputs #shape: [output language seq len, batch_size, target_vocab_size]
```

batch_size은  num_sentences가 한 배치 안에 있는 문장의 개수(배치 크기 batch size)를 나타낸다.

target_len은 target의 길이를 구하라는 것으로 출력 문장의 길이를 구하면 된다.

## 스페셜 피어세션
- transformer 공부 참고

https://nn.labml.ai/index.html

- 신박 Ai

https://www.youtube.com/@phdshinAI

- 위키독스

https://wikidocs.net/31379




