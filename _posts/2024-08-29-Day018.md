---
title: <Day018> Naver Boostcamp AI Tech
date: 2024-08-29
layout: post
tags: [Naver Boostcamp, daily report]
---

### 새롭게 배운 내용

One-hot vector -> Word Embedding

**One-hot Encoding**
단어들을 categorical variable로 one-hot vector로 나타내는 것
- vector의 각 demension을 vocabulary에 정의된 각 단어들로 생각.
	- 이 말이 되게 생소했다. 
	- 해당 단어에 해당하는 demension의 값을 1, 나머지 demension의 값들은 모두 0으로 설정.
- 혈액형 예시
	- A = [1 0 0 0]^T
	- B = [0 1 0 0]^T
	- AB = [0 0 1 0]^T
	- O = [0 0 0 1]^T
- 서로 다른 단어들의 내적 유사도(Dot-product similarity)는 항상 0이다. 단어들 간의 유클리드 거리도 항상 루트2
-> 단어의 의미가 유사하거나 관련이 없음을 전혀 표현하지 못하고 있다.

- 혈액형의 예시를 보면 소수의 demension만 0이 아닌 값을 가지고 있고 나머지는 대부분 0으로 이루어진 벡터로 단어를 표현하고 있다. 
	- 수많은 개수의 0을 컴퓨터 메모리에 저장하는 것 보다 어떤 곳에서 1인지를 나타내는 demension index만을 메모리에 저장하면 훨씬 효율적일 것이다. 
	- 꼭 one-hot vector가 아니여도 특정 값만 몇개의 demension이 표현하고 나머지는 0이라면 {(인덱스, 특정값), (인덱스, 특정값)} 이런식으로 표현할 수 있다.
	- Sparse representation
---
**Distributed Vector Representation**
Dense Vector : 0이 아닌 값이 빽빽히 차있다!
- 정해둔 특정한 개수의 차원을 가지는 벡터 형태
- 모든 값들이 non-zero의 실수 값을 가진다.
- 그러한 벡터로 서로 다른 단어들 간의 유클리드 거리, 내적, 코사인 유사도를 계산했을 때 의미론적인 유사성을 나타낸다. 
- Sparse representation과 반대되는 개념

**word embedding**
- one-hot vector로 나타낸 단어들을 새로운 형태의 dense vector로 표현하는 기법: word embedding
- word embedding의 대표적인 방법론 : Word2Vec
---
**Word2Vec**

word-embedding이 단어들 간의 의미 유사도를 잘 인코딩 하기 위해서는 어떤 단어들이 의미가 유사한지, 어떤 단어는 서로 무관한 지 정보가 필요하다. 
이러한 정보를 데이터를 기반으로! 유추할 수 있는 방법? -> 이 세상의 무수한 텍스트들(데이터)을 모았을 때 어떤 특정 단어를 중심으로 한 문장 내에서 동시에 함께 등장한 다른 단어들은 그 특정 단어와 유사도, 관련성이 높다고 볼 수 있다. 

실제로도 우리가
누군가와 얘기할 때 어떤 단어를 못 들었을 때, 어떤 단어를 모를 때 그 주변의 앞뒤의 문맥을 생각해서 해당 단위의 의미를 유추할 수 있게 된다.
- _이런 점에서 Dense Vector를 생각해낸 것 아닐까?_

핵심 IDEA
- 주변에 어떤 단어가 올지 앞뒤로 올 단어의 확률분포가 특정 단어의 의미를 결정한다!

1. CBoW(Continuous Bag of Words)
2. Skip-gram
	- 실제로 많이 쓰이는 것.
	- 중심단어가 입력으로 주어질 때 앞 뒤로 주변단어가 무엇일지를 예측하는 것.

주변단어를 예측하는 것을 two-layer neural net으로 나타낸다.
- **Input layer - Hidden layer - Output layer**
	- Hidden-layer는 사전보다 훨씬 작은 사이즈로 보통 설정된다.
	- output-layer에서는 정답값인 ground truth 단어를 예측하는 multiclass classification을 수행해야한다. 따라서 vocab 사이즈와 동일한 demension의 output layer를 얻어내고 이후 multiclass classification에 사용되는 최종 layer인 softmax를 통과해서 전체 voabulary에서 정의된 모든 단어들에 대한 예측된 확률분포를 얻어낸다.
	  softmax를 통해 합이 1인 확률분포 벡터가 만들어질 것이다.
	- Hidden layer에는 어떠한 non-linear도 적용하지 않는다.
		- non-linear없이 두개의 linear layer를 이어붙이게 되면 하나의 linear layer를 쓴 것과 동일하다고 볼 수도 있는데, hidden layer가 좀 더 작은 차원이기 때문에 또 꼭 선형과 같다고 볼순 없다.

학습데이터 : "I study math" 라는 한 문장이라면
1. word-level의 사전 구축 : {"math", "study", "I"}
2. window 사이즈에 따라 정답쌍을 구성하여 2-layer neural net을 학습한다.
	(I, study)
	(study, I)
	(study, math)
	(math, study)
3. (study, math)의 예시를 살펴보자.
	- 입력이 x : study에 해당하는 one-hot vector
	- x와 W1(2x3)을 곱해서 2차원 벡터를 만들어주고
		- 이 행렬곱의 결과는 W1의 사전 개수만큼의 column들 중 x(study)에 해당하는 두번째 column 벡터가 결과이다.
	- W2(3x2)를 곱해서 vocab과 같은 사이즈의 벡터를 만들어준다.
		- 앞서 구한 hidden layer와 W2와의 곱을 생각해볼 수 있다.
		- W2는 vocabulary 사이즈의 row 벡터를 가지고 있고, 각 row 벡터의 demension은 hidden layer의 demension과 동일하다.
		- W1에서 선택된 column 벡터와 W2의 각 row 벡터들이 내적을 수행해서 3차원 결과벡터가 만들어진다. 여기서 결과벡터에 해당하는 값이 내적했을때 +무한대로 최대값이 되어야한다.
4. 정답인 math의 one-hot vector (1, 0, 0)과 softmax의 output이 최대한 가깝도록 softmax loss를 통해 학습을 진행한다.

- W1, W2는 각 word별로 우리가 정해둔 demension으로 나타나는 word2vec의 output으로서의 densevector, distributed vector가 될 것이고
- W1에서는 입력단어벡터, W2는 출력단어의 벡터 -> 그들과의 내적 유사도에 기반해서 정답에 해당하는 내적값은 최대한 높여주기.
- 그러지 않은 것은 최대한 낮춰주기.

- 뒤에서는 embedding된(one-hot vector가 아닌) 벡터들을 입력값으로 주게 될 것이다.

---
**Word2vec의 특징인 Semantic Similarity의 예시 : Word Intrusion Detection**

- 주어진 텍스트에서 가장 이질적인 단어를 뽑아내는 것.
- 워드 임베딩을 통해 학습된 각각의 워드별 벡터를 대상으로 모든 pair wise 유클리드 거리를 구하고, 각 단어별로 나머지 단어들과의 평균적인 유클리드 거리가 가장 큰 단어를 구하면 가장 의미적으로 이질적인 단어를 고를 수 있다.
