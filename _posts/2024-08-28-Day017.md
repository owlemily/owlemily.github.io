---
title: <Day017> Naver Boostcamp AI Tech
date: 2024-08-28
layout: post
tags: [Naver Boostcamp, daily report]
---
오늘도 깃헙 특강 6시간이 있는날이다. 

### 새롭게 배운 내용

- 다양한 모델들의 등장배경, 기존 모델과의 차이점에 주목해서 공부하자.

**Tokenization**
- 단어나 character 단위로 주어진 텍스트를 쪼개는 과정.
- 주어진 텍스트를 어떤 기준으로 분리? -> Word-level, Character-level, Subword-level
- 각 타임스텝 마다 주어지는 하나하나의 입력 단위를 ==토큰==이라고 부른다.
- 토큰의 경우 Vocabulary, 사전이 정의되어있을 때 -> 사전 내에 정의된 하나의 categorial variable이라 볼 수 있고, 이를 one-hot vector로 표현할 수 있다.
사전(vocabulary)
- 사전이란, 주어진 학습 데이터 내에서 발견된 모든 종류의 유니크한 단어들을 다 모아서 각각을 하나하나의 특정 카테고리로 할당하고 이를 통해 학습데이터에서 나타난 각각의 토큰들을 이 사전에 기반한 categorical variable의 형태인 one-hot vector로 나타내는 과정.

Tokenization의 방식
- word-level
	- 띄어쓰기를 기준으로 구분하거나 형태소를 기준으로 구분한다. 공백 또한 하나의 입력 시퀀스로 구분된다.
	- 문제점:  학습이 완료된 모델을 대상으로 추론 단계에서는 새로운 입력 문장이 있었을 때 학습에서 보지 못한 단어가 나오면 사전에 정의되지 않은 단어이기 때문에 이를 적절한 one-hot vector 혹은 주어진 카테고리 내에 정의된 정보로 표현할 수 없게 된다. 이를 보완하기 위해서 학습단계에서 사전을 구축할 때 이후 테스트 단계에서 보지 못한 단어를 어떤 하나의 카테고리로 부여해서 one-hot vector로 인코딩하기 위해서 unkown 카테고리를 하나 만들어두고 사전에 정의되지 않은 단어가 들어오면 모두 unknown 토큰으로 처리된다. unknown 단어들의 경우 나름대로 특정한 의미를 가질 수 있을텐데 이를 적절히 나타내지 못하고 unkown으로 처리하면 딥러닝 모델이 학습수행에 있어서 방해요인이 있을 것이다. 
	- -> Out-of-vocabulary 문제 (OOV)
- character-level
	- 장점: 같은 알파벳을 공유하는 언어들: 영어나 스페인어 -> 공통적으로 문자단위의 tokenization이 수행될 수 있다. 
	  어떠한 단어도 A-Za-z 범위를 벗어날 수 없기 때문에 OOV문제를 해결
	- 단점: 단어단위의 tokenization에 비해 동일한 문장을 나타낼때 토큰의 개수가 지나치게 많아져서 딥러닝 모델이 처리해야하는 입력 시퀀스의 길이가 길어져서 계산량이 많아지거나 제한된 GPU 리소스로 긴 문장을 처리하는 것이 불가능할 수 있다.
	  주어진 문장을 토큰 단위로 인코딩할 때 각 토큰의 정보가 유의미한 정보를 내포하는 것이 좋을 텐데, character 자체가 유의미한 정보를 가진다고 보기 어렵다. 
	  풀고자하는 다양한 자연어처리task에 있어서 그렇게 높은 성능을 보여주진 못했다.
- subword-level\
	- 이슈: 특정 의미를 가진 subword들이 어떤 것들이 있는가에 대한 정보가 사전에  주어져 있지 않다는 것이다. 
	- 해결: 학습 데이터에서 빈번하게 나타나는 subword들을 중심으로 사전을 구축하는 방법을 생각해볼 수 있다. => Byte-pair Encoding(BPE)
	- 장점: character 단위의 tokenization보다는 시퀀스의 길이가 적다.
	  다수의 character들로 이루어진 subword도 포함할 뿐 아니라 character tokenization처럼 단일 캐릭터들도 사전 내에 기본적으로 포함되는 전략을 취하기 때문에 character-level tokenization처럼 단일 캐릭터들도 사전 내에 기본적으로 포함되는 전략을 취하므로 OOV 문제가 없다. 
	  OOV문제가 없으면서도 의미단위를 잘 나타내는 subword단위로 주어진 text를 tokenize했을 때 해당 자연어 모델이 다양한 task를 거쳐서 더 좋은 성능을 보여주고, 다양한 자연어처리 딥러닝모델에서 이 방법을 사용한다.
	- subword Tokenization 방법론에 따라 subword 단위는 다양하게 결정된다. 
---
**Byte Pair Encoding(BPE)** : subword-level tokenization의 대표 방법 중 하나
-> 높은 빈도수의 subword들을 중심으로 사전을 구축하는 것이 Byte Pair Encoding의 핵심 아이디어
1. 다양한 문장의 학습데이터에서 등장한 단어들의 횟수를 센다.
2. 단어들의 단일 character를 사전에 등록한다.
3. 길이가 2인 토큰 쌍을 만들어서 토큰쌍이 학습데이터 단어들에서 몇 번 나타났는지 빈도수를 센다. 
4. 가장 높은 빈도수를 보인 토큰 쌍을 사전에 추가적인 subword로 등록한다.
5. 새로 추가된 쌍까지 고려해서 새로운 단어 쌍을 만들고 빈도수를 학습데이터에서 계산한다.
6. 그렇게 해서 또 빈도수가 가장 높은 것을 새롭게 사전에 추가한다.
7. 계속해서 토큰 쌍 빈도수를 업데이트하고
8. 사전에 미리 정해둔 vocabulary size에 도달할 때까지 반복해준다. 
-> 단일 chracter + 긴길이의 subword들도 포함 -> tokenization할 수 있는 다양한 방법이 포함된 사전. 단일 character보다 긴 길이의 subword로 토큰화하는 것이 더 좋을 것이다. 따라서...

-> 주어진 입력텍스트를 Tokenization하는 방식은 사전에 등록된 길이가 가장 긴 토큰들부터 입력데이터가 등록되어있는지  확인. 

- 이 BPE 알고리즘은 GPT, Large Language Model에서 가장 많이 사용되는 알고리즘.
- 그 외에도 다른 subword-level tokenization 방법들이 존재한다.
