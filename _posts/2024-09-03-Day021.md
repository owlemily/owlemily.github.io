---
title: <Day021> Naver Boostcamp AI Tech
date: 2024-09-03
layout: post
tags: [Naver Boostcamp, daily report]
---

### 회고
- 위키톡스 자연어처리 스터디 자료를 읽으면서 스탠포드 강의에서 내가 아리송했던 부분이 명확히 이해됐다. 
- 스탠포드 강의에서 모르고 넘어갔던 부분도 다시 언급되는 것을 보고 이 스터디를 하길 잘했다는 생각이 든다. 
- 위키독스 스터디 이후에 스탠포트 강의를 다시 들어보면 더 깊이 있는 학습이 가능할 것 같다.
- Perplexity에 대해서 처음 알게 되었다. 

### 피어세션

- 강의 리뷰
- ppt  6강 리뷰
	- 14. Attention 시각화 치환된다했는데 여러 단어 프랑스에서는 세단어 -> 영어에서는 한 단어 -> 이게 attention으로 어떻게 수행이 되는지? 
	- 버리는 느낌으로 해결? 3개가 다 똑같이 나오면 출력도 environment가 3번 나와야하는건가? 각각의 단어에 대한 Attention 가중치가 나와야하는데 세개의 단어가ㅓ 어떻게 한개의 단어로 나오는 지
	- attention을 통해 불필요한 단어를 건너 뛰는 것이 가능.
- ppt 7강 리뷰
	- multi-head attention에서 같은방법으로 여러번 하는 것이잖아. 다양성이 증가한다는게 가중치 초기화에 따라 달라지는 건가?
		- 선형변환을 통해서 QVK를 넣는데 선형변환을 각각 다르게 한다는 것으로 이해했음.
		- 어떤 거는 관계성을 중시, 어떤거는 본인 스스로가 높게 나오는 것.
		- Wk가 헤드마다 다르거지
		- Wk도 backprop으로 추출되는거지. -> 그럼 특징이 알아서 추출이 되나???
		- I, came home, late에서 학습과정에서 넣는거 아닌가
- ppt 8강 리뷰
	- 4 -> 0.65 가 되는데 4에서 3.5를 빼고 여기서 1.1을 하라고 했는데 이 값이 안나오는 것 같아. 
		- 이거 왜그러냐면 표준화 표준편자 1.11, 편차가 3.5 
		- 표준화 
		- 모델 normalization할 때 문제가 되는 경우가 있어. 분모가 0이 안되게 입실론같은 값을 표준편차의 값을 양수의 값으로 나눠진단 말이야.
		- 10의 -5승정도를 더해서 나눠서 스케일링하곤 하는데 
		- 더 큰 값으로 나눠주면 0.5보다 작게 나와야하는데 

